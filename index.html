<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Project Page for Generative Visual Foresight Meets Task-Agnostic Pose Estimation in Robotic Table-top Manipulation.">
  <meta name="keywords" content="Embodied AI, Artificial Intelligence, Machine Learning, Diffusion Model, Decision Making, Robotics, Video Model, Behavior Cloning, Pose Estimation">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- TODO: -->
  <!-- TODO: -->
  <meta name="author" content="Chuye Zhang, Xiaoxiong Zhang, Wei Pan, Linfang Zheng, Wei Zhang">
  <!-- TODO: -->
  <!-- TODO: -->
  <title>Generative Visual Foresight Meets Task-Agnostic Pose Estimation in Robotic Table-top Manipulation</title>

  <!-- TODO: -->
  <!-- TODO: -->
  <!-- <meta name="google-site-verification" content="luiKJ6NTZHsgoD15xoPljBeAHAgSwMOoXhBmQG8-LQ8" /> -->
  <!-- TODO: -->
  <!-- TODO: -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <!-- <link rel="stylesheet" href="./static/css/bulma-carousel.min.css"> -->
  <!-- <link rel="stylesheet" href="./static/css/bulma-slider.min.css"> -->
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="bootstrap-grid.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="load-mathjax.js" async></script>

  <link rel="stylesheet" href="./static/css/splide.min.css">
  <script src="./static/js/splide.min.js"></script>
  <link rel="icon" type="image/x-icon" href="./materials/favicon.ico" />

</head>

<body>

<!-- TODO: -->
<!-- Navigation Bar -->

<!-- style="margin-bottom: 10px; padding-bottom: 10px" -->
<!-- style="margin-bottom: 0px; padding-bottom: 0px" -->
<section class="hero" style="background-color: #fbecdd;">
  <div class="hero-body hero-body-dec19" >
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- is-1 or is-2 -->
          <h1 class="title publication-title" style="font-size: 43px;">
            <!-- Grounding Video Models to Actions through Goal Conditioned Exploration</h1> -->
            Generative Visual Foresight Meets <br> 
            <span style="color: #974F9F;">Task-Agnostic</span> 
            <span style="color: #4A7CB4;">Pose</span> 
            <span style="color: #67AC55;">Estimation</span> 
              in Robotic Table-top Manipulation </h1>

            <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zhangchuye.github.io/">Chuye Zhang</a><sup>* 1</sup>,
            </span>
            <span class="author-block">
              <a href="">Xiaoxiong Zhang</a><sup>* 1</sup>
            </span>
              <span class="author-block">
                <a href="">Wei Pan</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="">Linfang Zheng</a><sup>† 2,3</sup>,
              </span>
              <span class="author-block">
                <a href="">Wei Zhang</a><sup>† 1,2</sup>
              </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <!-- <img src="static/Brown-logo-crop.png" width="20px"> -->
              <!-- <sup>1</sup>Georgia Institute of Technology, University -->
              <sup>1</sup>Southern University of Science and Technology,
              <sup>2</sup>LimX Dynamics,
              <sup>3</sup>The University of Hong Kong
            </span>
            <!-- &nbsp; -->
            <!-- <span class="author-block"> -->
              <!-- <img src="static/MIT_logo.svg" width="40px"> -->
              <!-- <sup>2</sup>MIT -->
            <!-- </span> -->

            <!-- <br>
            <img src="static/Brown-logo.png" width="110px">
            &nbsp;
            &nbsp;
            <img src="static/MIT-logo.svg" width="80px"> -->

            <br> CORL 2025 (Poster)

            <!-- <br>
            <img src="static/Brown-logo.png" width="190px">
            &nbsp;
            &nbsp;
            <img src="static/MIT-logo.svg" width="135px"> -->

          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv -->
              <span class="link-block">
                <!-- TODO: Add arXiv link -->
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv(Coming Soon)</span>
                </a>
              </span>

              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2411.07223"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->

              

              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/video-to-action/video-to-action-release"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Exploration Code</span>
                </a>
              </span> -->

              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-file-video"></i>
                  </span>
                  <span>Code(Coming Soon)</span>
                </a>
              </span>

              <!-- <span class="link-block">
                <a href="https://colab.research.google.com/drive/1pARD89PfSzF3Ml6virZjztEJKWhXVkBY?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img width="20px" src="static/google-colab-32.png">
                  </span>
                  <span>Colab</span>
                </a>
              </span> -->

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <!-- <div class="column is-full-width"> -->
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>

            Robotic manipulation in unstructured environments requires systems that can generalize across diverse tasks while maintaining robust and reliable performance. 
            We introduce GVF-TAPE, a closed-loop framework that combines generative visual foresight with task-agnostic pose estimation to enable scalable robotic manipulation. 
            GVF-TAPE employs a generative video model to predict future RGB-D frames from a single side-view RGB image and a task description, offering visual plans that guide robot actions. 
            A decoupled pose estimation model then extracts end-effector poses from the predicted frames, translating them into executable commands via low-level controllers. 
            By iteratively integrating video foresight and pose estimation in a closed loop, GVF-TAPE achieves real-time, adaptive manipulation across a broad range of tasks. 
            Extensive experiments in both simulation and real-world settings demonstrate that our approach reduces reliance on task-specific action data and generalizes effectively, providing a practical and scalable solution for intelligent robotic systems.
            
          </p>
        </div>


        <!-- <figure>
          <img width="75%" src="materials/1-teaser-v4-for-web.png" style="min-width: 200px">
        <p class="caption">
            <b>Method Overview.</b>
            Our approach learns to ground a large pretrained video model into continuous actions through goal-directed exploration in an environment. 
            Given a synthesized video, a goal-conditioned policy attempts to reach each visual goal in the video, 
            with data in the resulting real-world execution saved in a replay buffer to train the goal-conditioned policy.
        </p>
      </figure> -->

      </div>
    </div>
  </div>
</section>

<hr>














<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified">
          <p>
            We present GVF-TAPE, a real-time manipulation framework that decouples visual planning from action execution by combining generative video prediction with task-agnostic pose estimation. Unlike prior methods, GVF-TAPE learns from unlabeled videos and random exploration, removing the need for action-labeled data. This design allows robots to predict future visual outcomes and infer executable poses, enabling robust closed-loop control across diverse tasks. Experiments in both simulation and the real world show that GVF-TAPE outperforms action-supervised and video-based baselines, demonstrating the potential of label-free, foresight-driven frameworks for scalable manipulation. We hope this work encourages further research in video-guided, action-free robot learning.

            
          </p>
        </div>
      </div>
    </div>
  </div>
</section>





<!-- TODO: IMPORTANT -->
<!-- <hr>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content column is-full-width ">
    <h2 class="title is-4">BibTeX</h2>
    <pre><code>@article{luo2024grounding,
      title={Grounding Video Models to Actions through Goal Conditioned Exploration},
      author={Luo, Yunhao and Du, Yilun},
      journal={arXiv preprint arXiv:2411.07223},
      year={2024}
    }
</code></pre>
  </div>
</section> -->



<!-- <section class="section">
        <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Related Projects</h2>
          Check out a list of our related papers on compositional generation and energy based models. A full list can be found <a href="https://energy-based-model.github.io/Energy-based-Model-MIT/">here</a>!
          <br>
          <br>
        <div class="row vspace-top">
        <div class="col-sm-3">
            <video width="100%" playsinline="" autoplay="" preload="" muted="">
                <source src="materials/related/recyle.m4v" type="video/mp4">
            </video>
        </div>
        <div class="col-sm-9">
          <div class="paper-title">
            <a href="https://energy-based-model.github.io/reduce-reuse-recycle/">Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC</a>
        </div>
        <div>
            We propose new samplers, inspired by MCMC, to enable successful compositional generation. Further, we propose an energy-based parameterization of diffusion models which enables the use of new compositional operators and more sophisticated, Metropolis-corrected samplers.
        </div>
        </div>
        </div>
          <br>




        <div class="row vspace-top">
        <div class="col-sm-3">
            <video width="100%" playsinline="" autoplay="" preload="" muted="">
                <source src="materials/related/teaser_glide.mp4" type="video/mp4">
            </video>
        </div>
        <div class="col-sm-9">
          <div class="paper-title">
            <a href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/">Compositional Visual Generation with Composable Diffusion Models</a>
        </div>
        <div>
            We present a method to compose different diffusion models together, drawing on the close connection of
            diffusion models with EBMs. We illustrate how compositional operators enable
            the ability to composing multiple sets of objects together as well as generate images subject to
            complex text prompts.
        </div>
        </div>
        </div>
        <br>


        

        <div class="row vspace-top">
        <div class="col-sm-3">
            <div class="move-down">
                <img src="materials/related/diffuser_schematic-01.png" class="img-fluid" alt="comp_carton" style="width:100%">
            </div>
        </div>
        <div class="col-sm-9">
          <div class="paper-title">
            <a href="https://diffusion-planning.github.io/">Planning with Diffusion for Flexible Behavior Synthesis</a>
        </div>
        <div>
          Diffuser is a denoising diffusion probabilistic model that plans by iteratively refining randomly sampled noise. The denoising process lends itself to flexible conditioning, by either using gradients of an objective function to bias plans toward high-reward regions or conditioning the plan to reach a specified goal.
        </div>
        </div>
        </div>
      </div>
    </div>
        </div>
</section> -->





<footer class="footer">
  <div class="container">
    <!-- TODO: -->
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2411.07223">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/video-to-action/video-to-action-release" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>

    <div class="columns is-centered">
      <div class="column is-8 has-text-centered">
        <div class="content">
          <p>
            This website is forked from the
						<a href="https://nerfies.github.io/">Nerfies</a> and
						<a href="https://github.com/nerfies/nerfies.github.io">source code</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script src="./static/js/last_run.js"></script>

</body>
</html>
